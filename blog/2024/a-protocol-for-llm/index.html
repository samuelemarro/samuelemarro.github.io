<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>A Protocol Sketch For LLM Communication | Samuele Marro</title> <meta name="author" content="Samuele Marro"/> <meta name="description" content="So, how do we make LLMs talk to each other?"/> <meta name="keywords" content="adversarial-attack, artificial-intelligence, machine-learning"/> <meta property="og:site_name" content="Samuele Marro"/> <meta property="og:type" content="website"/> <meta property="og:title" content="Samuele Marro | A Protocol Sketch For LLM Communication"/> <meta property="og:url" content="https://samuelemarro.it/blog/2024/a-protocol-for-llm/"/> <meta property="og:description" content="So, how do we make LLMs talk to each other?"/> <meta property="og:locale" content="en"/> <meta name="twitter:card" content="summary"/> <meta name="twitter:title" content="A Protocol Sketch For LLM Communication"/> <meta name="twitter:description" content="So, how do we make LLMs talk to each other?"/> <meta name="twitter:site" content="@MarroSamuele"/> <meta name="twitter:creator" content="@MarroSamuele"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/icon.svg"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://samuelemarro.it/blog/2024/a-protocol-for-llm/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Samuele </span>Marro</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">A Protocol Sketch For LLM Communication</h1> <p class="post-meta">April 7, 2024</p> <p class="post-tags"> <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/category/ai"> <i class="fas fa-tag fa-sm"></i> ai</a>   </p> </header> <article class="post-content"> <h1 id="tldr">TL;DR</h1> <ul> <li>Natural language is flexible, structured data is efficient</li> <li>A protocol that supports structured data but allows natural language as a default achieves the best of both worlds</li> <li>Machines can use documents to describe highly specific communication protocols, which can be handled by traditional code routines</li> <li>For everything else (including negotiating new protocols, handling code failures and writing new routines) we can use LLMs</li> </ul> <h1 id="introduction">Introduction</h1> <p>About 1.5 years ago I made a blog post where I stated that natural language was the natural choice to allow flexible, programmer-less interfacing between machines. In that time span, a lot happened! GPT-4 was released and made a lot of people question their definitions of general intelligence. Llama and Mistral showed that good models aren’t necessarily closed source. And, most importantly, there has been a decisive trend towards bigger, more expensive and more centralized models, although the advances in quantization and MoE have somewhat slowed the growth.</p> <p>At the moment of writing, all LLMs that have comparable performance to the “big guys” (ChatGPT, Claude, Bard) are 70B models, which have requirements way beyond those the average consumer can afford. For example, <a href="https://qwenlm.github.io/blog/qwen1.5/" target="_blank" rel="noopener noreferrer">Qwen-1.5-72B</a> (#9 in the <a href="https://chat.lmsys.org/" target="_blank" rel="noopener noreferrer">Chatbot Arena</a>) requires, even with quantization, <a href="https://qwen.readthedocs.io/en/latest/benchmark/hf_infer.html" target="_blank" rel="noopener noreferrer">two NVIDIA A100 80 GB</a>, which puts a the cost of running Qwen locally at about 40k USD.</p> <p>So, how can we make open source LLMs more available to the general public?</p> <p>I think that the best tool to compete with massive LLMs is to use networks of (relatively) small LLMs. Having 10, 100 or 1000 LLMs talking to each other is definitely less efficient compared to just training a model that is 10, 100 or 1000 times larger, but it also means that you don’t need an entire data center just to do inference.</p> <p>And so, while I’m here in Oxford for my research visit, I thought it would be a good chance to expand upon the idea further, with the goal of building a concrete first step towards networks of LLMs. And I’d argue that the first step towards a network is how nodes in such a network communicate. Specifically, what I’m interested in is a simple protocol to allow communication between (LLM-powered) machines. Rather than directly outline the protocol I have in mind, I’ll walk you through the reasoning that led me to this specific implementation.</p> <h1 id="a-sample-task">A Sample Task</h1> <p>We’re going to use a very simple task: querying the price of a stock. There are two machines, Alice and Bob, each having their own databases, with their own schemas and their own conventions:</p> <p><img src="https://i.imgur.com/zuFelp4.png"></p> <p>Alice wants to query Bob to obtain the current price of MSFT, which at the time of writing sits at 425.52 USD.</p> <p>We have two seemingly competing goals:</p> <ul> <li>Flexibility: the machines should be able to adapt to changes in data, schema or goals with little effort;</li> <li>Efficiency: performing a task should be as fast and low-resource as possible, especially if it is performed multiple times.</li> </ul> <h1 id="level-0-manual-implementation">Level 0: Manual Implementation</h1> <p>The most standard approach for answering a query is for someone to expose an API for Bob, while someone else writes some code for Alice to query the API, convert it into a database-friendly schema and store it.</p> <p>This is as efficient as it gets: there’s a reason if pretty much all of machine-to-machine communication is performed over APIs. But it’s not flexible: if you want to use a different API, or if you want to query more information (e.g. the price-to-earnings ratio), or even if you just want to change the internal schema of your database, you need a human to implement the changes.</p> <h1 id="level-1-natural-language">Level 1: Natural Language</h1> <p>But it’s 2024, and LLMs are hot. Since LLMs can solve everything, let’s just build two LLMs that chat with each other!</p> <p>Specifically, we add two LLMs that can interface with Alice’s and Bob’s databases, respectively. Both LLMs are capable of using natural language to communicate with each other:</p> <p><img src="https://i.imgur.com/rc26z8X.png"></p> <p>This is as flexible as it gets: if Bob’s database changes schema, Bob’s LLM only needs to be informed of the new schema, while from the point of view of Alice nothing changed. If Alice wants to query the price-to-earnings ratio, it’s just a matter of changing the question. If Bob’s server goes down, Alice can start querying another natural language-friendly machine without any major disruptions.</p> <p>At the same time, however, we are using two language models to send a floating point value between two databases. This is the computational equivalent of shooting a fly with a bazooka.</p> <h1 id="level-2-reinventing-apis">Level 2: Reinventing APIs</h1> <p>Wouldn’t it be just easier if we could have the LLMs agree on a way to send the data? For example, suppose that Alice’s query is:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Alice: What's the price of MSFT? Send the data as a JSON with a single field, "price", containing the price of the answer
Bob: { "price" : 425.52 }
</code></pre></div></div> <p>If Bob is sufficiently consistent with its replies, Alice can just write a simple routine to store the data:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>const result = await Alice.queryPrice('MSFT')
const price = JSON.parse(result).price
await myDatabase.update({ 'MSFT' : price })
</code></pre></div></div> <p>This means that we don’t need to use Alice to store the result in the database: we can rely on good ol’ code, written by the LLM. Similarly, Bob can ask Alice to provide queries in a standard format, so that it can also use routines. So, the first communication might be something like:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Alice: I want to establish a protocol between us. I want to know the price of a stock.
Bob: I can send you the price as an XML, something like &lt;xml&gt;The price of &lt;stockName&gt;MSFT&lt;/stockName&gt; is &lt;stockPrice&gt;425.52&lt;/stockPrice&gt;&lt;/xml&gt;
Alice: Can we use JSON instead?
Bob: Ok, how about you send me a JSON with the price query, e.g. { "priceQuery" : "MSFT" }, and I send you a JSON with the result, e.g. { "price" : 425.52 } ?
Alice: Sounds good!
</code></pre></div></div> <p>After this natural-language negotiation, future communication might be something much more terse:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Alice: { "priceQuery" : "MSFT" }
Bob: { "price" : 425.52 }
</code></pre></div></div> <p>In this case, Alice and Bob could just use the routines written by the LLMs, which means that the latter don’t need to be invoked.</p> <p>In order to ensure that Alice and Bob are both sure that they’re using the same protocol, we might add an identifier for the protocol, plus an ACK/NAK token to programmatically check if the other party understood which protocol they’re using:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Alice: ALICE-BOB-STOCK-PRICING { "priceQuery" : "MSFT" }
Bob: ACK { "price" : 425.52 }
</code></pre></div></div> <p>Congratulations, we’ve just reinvented APIs. However, this approach has several benefits over a regular API:</p> <ul> <li>If either Alice or Bob need to change the protocol, they can just negotiate a new one</li> <li>If Alice needs to make some unusual queries, it can just ask in natural language</li> </ul> <p>In general, the cool thing about this approach is that natural language is a universally supported default for communication. If there’s a more efficient protocol, that’s great! But if there are issues or unexpected changes, natural language is still an option.</p> <p><img src="https://i.imgur.com/b2wfxqF.png"></p> <h1 id="level-3-scaling-to-a-network">Level 3: Scaling to a Network</h1> <p>Our protocol is perfectly fine for communication between two machines. But when we scale to a network of machines, there are three new problems:</p> <ul> <li>Negotiating a protocol with each machine in the network is inefficient</li> <li>If a query is forwarded to a different machine, it might lack the context required to understand the query</li> <li>There might be two protocols with the same name, or a protocol with two different names</li> </ul> <p>Fortunately, we humans have created a tool to efficiently tell the rest of the world how to implement a protocol: we call them standards! Whether they’re RFCs, ISOs, ERCs or whatever, these documents provide everything two parties need to initiate a communication.</p> <p>So, for our stock market info, we can just write a document:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The client sends a plaintext JSON document having a single field, "priceQuery", which is the ticker of the stock (in uppercase). The server replies by sending a JSON document having a single field, "price", which is the price (in USD) of the corresponding stock. This price is expressed as a floating-point value.
</code></pre></div></div> <p>Note that:</p> <ul> <li>Unlike a schema, this document also provides the semantics for the data, which means that a model that has never seen this document before can still understand the data that was received;</li> <li>The document can range from general-purpose communication formats to highly specialized data formats;</li> <li>LLMs can write these documents.</li> </ul> <p>Now we just need a way to assign a unique identifier to this document. The classic approach would be to rely on a standards agency to assign codes, but: a. That defeats the whole point of having a decentralized system; b. No agency (even with the help of LLMs) could process and catalogue all the possible communication protocols that two LLMs might establish.</p> <p>There is, however, an alternative, inspired by the <a href="https://ipfs.tech/" target="_blank" rel="noopener noreferrer">IPFS Protocol</a>: using the hash of the document as a unique identifier. By definition, a document can only have one hash, and with a good enough hashing scheme the probability of collisions is so low (e.g. 1 over 2^160, if we’re using SHA1) that they might as well be non-existant.</p> <p>And that’s it! The final communication, assuming that we’re using Base64-encoded SHA1 as hashing protocol, is thus something like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Alice: o2R8vS9V7BqfhHQFVWapyCVyqNs= { "priceQuery" : "MSFT" }
Bob: ACK { "price" : 425.52 }
</code></pre></div></div> <p>All of this happens without the intervention of the LLMs: if, however, there are any issues with the communication, the LLMs can simply intervene. For example, in case Bob doesn’t have a routine to handle the protocol, it can just send a NAK, which makes Alice send the corresponding protocol (which is then read by Bob’s LLM). Similarly, if for some reason the routine fails, the LLM can take over. As a flowchart:</p> <p><img src="https://i.imgur.com/fNvqXCD.png"></p> <h1 id="conclusion">Conclusion</h1> <p>This pretty much covers the core of my idea for the communication protocol. Of course, this is by far not a full-fledged proposal, as there are lots of open questions, such as:</p> <ul> <li>How do we broadcast information about protocols to the rest of the network?</li> <li>How can a machine trust a previously-unknown protocol to be safe?</li> <li>How can we implement some common features of other communication protocols (e.g. authentication, retrying, message forwarding…)?</li> <li>Is there a preferrable way to write a protocol document?</li> <li>Which hashing algorithm should be used?</li> </ul> <p>I hope to cover potential solutions to these challenges in the next posts. That said, if you have any opinions on the protocol, its strengths and its flaws, feel free to reach out! The more I think about LLM communication, the more it looks like building a proper ecosystem (protocols, software, know-how…) for networks of LLMs is something that will require the effort of an entire community. Fortunately, it looks like quite a lot of people seem to care about keeping Machine Learning decentralized.</p> <p>Edit: It turns out that there are indeed lots of people who care about decentralized AI! In the meantime, I and some other folks at Oxford refined this idea and made <a href="https://agoraprotocol.org/" target="_blank" rel="noopener noreferrer">Agora</a>. Check out the <a href="https://arxiv.org/abs/2410.11905" target="_blank" rel="noopener noreferrer">paper</a> as well.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Samuele Marro. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>